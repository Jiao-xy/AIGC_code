import ollama
import re
import spacy
from collections import Counter

# åŠ è½½ spaCy è‹±è¯­æ¨¡å‹ï¼ˆç”¨äºè¯­æ³•åˆ†æï¼‰
nlp = spacy.load("en_core_web_sm")

# éœ€è¦é‡ç»„çš„æ–‡æœ¬
input_text = "storages, problems low-density message belief-propagation pre-processing for flash To solve the with a algorithm codes is NAND (LDPC) parity-check the decoding binary of for proposed. data (VNBP-MP) reliability variable-node-based"
input_text="the the A playing listens audience man piano quietly."
# è®¡ç®—åŸå§‹æ–‡æœ¬çš„å•è¯æ•°
def word_count(text):
    return len(text.split())

original_word_count = word_count(input_text)

# æ”¹è¿›åçš„ Promptï¼Œç¡®ä¿ä¸æ”¹å˜é•¿åº¦ï¼Œä¸å¢åŠ æˆ–åˆ é™¤å•è¯
prompt = f"""
Reorganize the following text while keeping the original sentence length and structure intact. 
Ensure grammatical correctness, improve readability, and maintain all technical terms and logical flow exactly as in the original.
Do not add or remove words excessively, and avoid unnecessary rewording.
Only adjust word order for better readability.

Text: {input_text}
"""

# ä½¿ç”¨ System Prompt ç¦æ­¢ DeepSeek è§£é‡Š
messages = [
    {"role": "system", "content": "You must only return the reorganized text. Do not explain, do not analyze, do not add extra words."},
    {"role": "user", "content": prompt}
]

# ç›®æ ‡ï¼šè‡³å°‘æ”¶é›† 10 ä¸ªç¬¦åˆæ‰€æœ‰è¦æ±‚çš„å¥å­
num_target_sentences = 10
collected_sentences = []
iteration_count = 0

while len(collected_sentences) < num_target_sentences:
    iteration_count += 1
    print(f"ç¬¬ {iteration_count} æ¬¡å°è¯•ç”Ÿæˆå¥å­...")

    response = ollama.chat(model='deepseek-r1:7b', messages=messages)
    generated_text = response['message']['content']

    # å»æ‰ <think>...</think> ä¹‹é—´çš„å†…å®¹
    cleaned_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL).strip()
    
    # ä»…ä¿ç•™å•è¡Œæ–‡æœ¬
    cleaned_text = cleaned_text.replace("\n", " ").strip()
    
    # **1ï¸âƒ£ åœ¨ç”Ÿæˆæ—¶æ£€æµ‹å­—æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚**
    word_diff = word_count(cleaned_text) - original_word_count
    if not (-2 <= word_diff <= 3):  # å…è®¸å•è¯æ•°å˜åŒ–èŒƒå›´ï¼š-5 åˆ° +10
        print(f"âŒ å­—æ•°ä¸ç¬¦åˆè¦æ±‚ ({word_diff})ï¼Œè·³è¿‡")
        continue
    
    # **2ï¸âƒ£ è¯­æ³•å®Œæ•´æ€§æ£€æµ‹**
    def is_valid_sentence(sentence):
        """ä½¿ç”¨ spaCy è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯­æ³•æ£€æµ‹"""
        doc = nlp(sentence)

        # ç¡®ä¿å¥å­é•¿åº¦é€‚ä¸­
        if len(doc) < 5:
            print("âŒ è¯­æ³•é”™è¯¯: å¥å­è¿‡çŸ­")
            return False

        # ç¡®ä¿å­˜åœ¨åŠ¨è¯ï¼ˆè°“è¯­ï¼‰
        if not any(token.dep_ == "ROOT" for token in doc):
            print("âŒ è¯­æ³•é”™è¯¯: ç¼ºå°‘ä¸»è°“ç»“æ„")
            return False
        
        # è®¡ç®—å•è¯é¢‘ç‡ï¼Œé¿å…æ— æ„ä¹‰é‡å¤
        word_freq = Counter(word.text.lower() for word in doc if word.is_alpha)
        if max(word_freq.values()) > len(doc) * 0.5:  # å¦‚æœæŸä¸ªå•è¯å æ¯”è¶…è¿‡ 50%ï¼Œå¯èƒ½æ˜¯æ— æ„ä¹‰çš„å¥å­
            print("âŒ è¯­æ³•é”™è¯¯: å¥å­ä¸­æŸä¸ªå•è¯é‡å¤è¿‡å¤š")
            return False

        return True
    
    if not is_valid_sentence(cleaned_text):
        continue  # è¯­æ³•ä¸å®Œæ•´ï¼Œè·³è¿‡
    
    # **3ï¸âƒ£ å¯è¯»æ€§è¯„ä¼°**
    def readability_score(sentence):
        """è®¡ç®—å¥å­çš„å¯è¯»æ€§è¯„åˆ†ï¼ˆæ›´ç®€æ´ã€æ›´æœ‰é€»è¾‘çš„å¥å­å¾—åˆ†æ›´é«˜ï¼‰"""
        doc = nlp(sentence)
        avg_word_length = sum(len(token.text) for token in doc) / len(doc)
        num_commas = sum(1 for token in doc if token.text == ',')
        score = 1.0 - (num_commas / len(doc)) - (avg_word_length / 10)  # é€‚å½“å¹³è¡¡æ ‡ç‚¹å’Œå•è¯é•¿åº¦
        
        # ç¡®ä¿å¥å­æµç•…ï¼Œä¸è¦å¤ªçŸ­æˆ–å†—é•¿
        if len(doc) < 8 or len(doc) > 40:
            print("âŒ å¯è¯»æ€§å¤ªä½: å¥å­è¿‡çŸ­æˆ–è¿‡é•¿")
            return 0  # ç›´æ¥å‰”é™¤
        
        return score
    
    read_score = readability_score(cleaned_text)
    
    if read_score < 0.3:  # å¦‚æœå¯è¯»æ€§å¤ªå·®ï¼Œè·³è¿‡
        print("âŒ å¯è¯»æ€§å¤ªå·®ï¼Œè·³è¿‡")
        continue

    # **4ï¸âƒ£ é€šè¿‡æ‰€æœ‰ç­›é€‰ï¼ŒåŠ å…¥å€™é€‰é›†**
    collected_sentences.append((cleaned_text, read_score))
    print(f"âœ… å¥å­é€šè¿‡ (è¯„åˆ†: {read_score:.2f}): {cleaned_text}")

# ================= æŒ‰å¯è¯»æ€§æ’åº =================
collected_sentences.sort(key=lambda x: x[1], reverse=True)

# ================= è¾“å‡ºæœ€ç»ˆç»“æœ =================
print("\nğŸ”¹ åŸæ–‡ï¼š", input_text)
print("\nâœ… ç”Ÿæˆçš„æœ€ä½³å¥å­ï¼ˆæŒ‰å¯è¯»æ€§æ’åºï¼‰ï¼š")
for i, (sentence, score) in enumerate(collected_sentences, 1):
    print(f"{i}. (è¯„åˆ†: {score:.2f}) {sentence}")
