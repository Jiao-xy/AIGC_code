import openai
import re
import spacy
from collections import Counter

# åŠ è½½ spaCy è‹±è¯­æ¨¡å‹ï¼ˆç”¨äºè¯­æ³•åˆ†æï¼‰
nlp = spacy.load("en_core_web_sm")

# éœ€è¦é‡ç»„çš„æ–‡æœ¬
input_text = "storages, problems low-density message belief-propagation pre-processing for flash To solve the with a algorithm codes is NAND (LDPC) parity-check the decoding binary of for proposed. data (VNBP-MP) reliability variable-node-based"

# è®¡ç®—åŸå§‹æ–‡æœ¬çš„å•è¯æ•°
def word_count(text):
    return len(text.split())

original_word_count = word_count(input_text)

# DeepSeek API è®¾ç½®
api_key = ""  # è¯·æ›¿æ¢ä¸ºä½ çš„ API Key
with open("/home/jxy/Data/deepseek_api_key.txt", "r") as file:
    api_key = file.readline().strip()  # è¯»å–ç¬¬ä¸€è¡Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼å’Œæ¢è¡Œç¬¦
base_url = "https://api.deepseek.com"

# æ¨èæ¨¡å‹ï¼šdeepseek-chatï¼ˆæ›´ç»æµï¼‰
model = "deepseek-chat"

# æç¤ºè¯
prompt = f"""
Reorganize the following text while keeping the original sentence length and structure intact.
Ensure grammatical correctness, improve readability, and maintain all technical terms.
Do not add or remove words excessively. Only adjust word order for better clarity.

Text: {input_text}
"""

# OpenAI å…¼å®¹çš„ API å®¢æˆ·ç«¯
client = openai.OpenAI(api_key=api_key, base_url=base_url)

# ç›®æ ‡ï¼šè‡³å°‘æ”¶é›† 10 ä¸ªç¬¦åˆæ‰€æœ‰è¦æ±‚çš„å¥å­
num_target_sentences = 3
collected_sentences = []
iteration_count = 0

while len(collected_sentences) < num_target_sentences:
    iteration_count += 1
    print(f"ç¬¬ {iteration_count} æ¬¡å°è¯•ç”Ÿæˆå¥å­...")

    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are an AI assistant specialized in text restructuring."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.5,  # æ§åˆ¶éšæœºæ€§
        max_tokens=150,  # é™åˆ¶è¾“å‡º Token æ•°
        stream=False
    )

    # è§£æ API å“åº”
    if response.choices:
        generated_text = response.choices[0].message.content
    else:
        print("âŒ API æœªè¿”å›æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
        continue

    # ç›´æ¥ä½¿ç”¨ API è¿”å›çš„å†…å®¹
    cleaned_text = generated_text.strip()

    # **1ï¸âƒ£ åœ¨ç”Ÿæˆæ—¶æ£€æµ‹å­—æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚**
    word_diff = word_count(cleaned_text) - original_word_count
    if not (-5 <= word_diff <= 10):  # å…è®¸å•è¯æ•°å˜åŒ–èŒƒå›´ï¼š-5 åˆ° +10
        print(f"âŒ å­—æ•°ä¸ç¬¦åˆè¦æ±‚ ({word_diff})ï¼Œè·³è¿‡")
        continue

    # **2ï¸âƒ£ è¯­æ³•å®Œæ•´æ€§æ£€æµ‹**
    def is_valid_sentence(sentence):
        """ä½¿ç”¨ spaCy è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯­æ³•æ£€æµ‹"""
        doc = nlp(sentence)

        # ç¡®ä¿å¥å­é•¿åº¦é€‚ä¸­ï¼ˆé€‚å½“æ”¾å®½ï¼‰
        if len(doc) < 5 or len(doc) > 60:  # ä»¥å‰æ˜¯ 8-40ï¼Œç°åœ¨æ”¹ä¸º 5-60
            print(f"âŒ è¯­æ³•é”™è¯¯: å¥å­é•¿åº¦ ({len(doc)}) ä¸ç¬¦åˆèŒƒå›´")
            return False

        # ç¡®ä¿å­˜åœ¨åŠ¨è¯ï¼ˆè°“è¯­ï¼‰
        if not any(token.dep_ == "ROOT" for token in doc):
            print("âŒ è¯­æ³•é”™è¯¯: ç¼ºå°‘ä¸»è°“ç»“æ„")
            return False
        
        return True
    
    if not is_valid_sentence(cleaned_text):
        continue  # è¯­æ³•ä¸å®Œæ•´ï¼Œè·³è¿‡

    # **3ï¸âƒ£ å¯è¯»æ€§è¯„ä¼°**
    def readability_score(sentence):
        """è®¡ç®—å¥å­çš„å¯è¯»æ€§è¯„åˆ†ï¼ˆæ›´ç®€æ´ã€æ›´æœ‰é€»è¾‘çš„å¥å­å¾—åˆ†æ›´é«˜ï¼‰"""
        doc = nlp(sentence)
        avg_word_length = sum(len(token.text) for token in doc) / len(doc)
        num_commas = sum(1 for token in doc if token.text == ',')
        
        # é€‚å½“æ”¾å®½å¯è¯»æ€§è¯„åˆ†çš„æ ‡å‡†
        score = 1.0 - (num_commas / len(doc)) - (avg_word_length / 15)  # ä»¥å‰æ˜¯ /10ï¼Œç°åœ¨æ”¹ä¸º /15
        
        return score
    
    read_score = readability_score(cleaned_text)
    
    if read_score < 0.3:  # ä»¥å‰æ˜¯ 0.5ï¼Œç°åœ¨é™ä½åˆ° 0.3
        print(f"âŒ å¯è¯»æ€§å¤ªå·® ({read_score:.2f})ï¼Œè·³è¿‡")
        continue

    # **4ï¸âƒ£ é€šè¿‡æ‰€æœ‰ç­›é€‰ï¼ŒåŠ å…¥å€™é€‰é›†**
    collected_sentences.append((cleaned_text, read_score))
    print(f"âœ… å¥å­é€šè¿‡ (è¯„åˆ†: {read_score:.2f}): {cleaned_text}")

# ================= æŒ‰å¯è¯»æ€§æ’åº =================
collected_sentences.sort(key=lambda x: x[1], reverse=True)

# ================= è¾“å‡ºæœ€ç»ˆç»“æœ =================
print("\nğŸ”¹ åŸæ–‡ï¼š", input_text)
print("\nâœ… ç”Ÿæˆçš„æœ€ä½³å¥å­ï¼ˆæŒ‰å¯è¯»æ€§æ’åºï¼‰:")
for i, (sentence, score) in enumerate(collected_sentences, 1):
    print(f"{i}. (è¯„åˆ†: {score:.2f}) {sentence}")
