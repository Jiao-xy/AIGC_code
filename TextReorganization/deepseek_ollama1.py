import ollama
import re
import spacy
from collections import Counter
from difflib import SequenceMatcher

# åŠ è½½ spaCy è‹±è¯­æ¨¡å‹ï¼ˆç”¨äºè¯­æ³•åˆ†æï¼‰
nlp = spacy.load("en_core_web_sm")

# éœ€è¦é‡ç»„çš„æ–‡æœ¬
input_text = "storages, problems low-density message belief-propagation pre-processing for flash To solve the with a algorithm codes is NAND (LDPC) parity-check the decoding binary of for proposed. data (VNBP-MP) reliability variable-node-based"

# è®¡ç®—åŸå§‹æ–‡æœ¬çš„å•è¯æ•°
def word_count(text):
    return len(text.split())

original_word_count = word_count(input_text)

# æ”¹è¿›åçš„ Promptï¼Œç¡®ä¿ä¸æ”¹å˜é•¿åº¦ï¼Œä¸å¢åŠ æˆ–åˆ é™¤å•è¯
prompt = f"Reorganize the following text while keeping the original sentence length and structure intact. Do not add or remove words. Maintain all technical terms and logical flow exactly as in the original. Only adjust word order for better readability.: {input_text}"

# ä½¿ç”¨ System Prompt ç¦æ­¢ DeepSeek è§£é‡Š
messages = [
    {"role": "system", "content": "You must only return the reorganized text. Do not explain, do not analyze, do not add extra words."},
    {"role": "user", "content": prompt}
]

# ç›®æ ‡ï¼šè‡³å°‘æ”¶é›† 10 ä¸ªç¬¦åˆå•è¯æ•°è¦æ±‚çš„å¥å­
num_target_sentences = 10
generated_sentences = []
number=0
while len(generated_sentences) < num_target_sentences:
    number+=1
    print(f"ç¬¬{number}æ¬¡å¾ªç¯")
    response = ollama.chat(model='deepseek-r1:7b', messages=messages)
    generated_text = response['message']['content']
    
    # å»æ‰ <think>...</think> ä¹‹é—´çš„å†…å®¹
    cleaned_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL).strip()
    
    # ä»…ä¿ç•™å•è¡Œæ–‡æœ¬
    cleaned_text = cleaned_text.replace("\n", " ").strip()
    
    # **åœ¨ç”Ÿæˆæ—¶æ£€æµ‹å•è¯æ•°æ˜¯å¦å˜åŒ–å¤ªå¤§**
    if cleaned_text and abs(word_count(cleaned_text) - original_word_count) <= 10:  # å…è®¸æœ€å¤š Â±3 è¯çš„å˜åŒ–
        generated_sentences.append(cleaned_text)
    else:
        print("å­—æ•°ä¸ç¬¦åˆ")
        print(f"å­—æ•°å˜åŒ–{word_count(cleaned_text) - original_word_count}ï¼Œ\nç”Ÿæˆæ–‡æœ¬å†…å®¹ï¼š{cleaned_text}")

# ================= è¿‡æ»¤æ‰æ˜æ˜¾é”™è¯¯çš„å¥å­ï¼ˆåŸºäº spaCy è¯­æ³•åˆ†æï¼‰ =================
def is_valid_sentence(sentence):
    """ä½¿ç”¨ spaCy æ£€æµ‹å¥å­æ˜¯å¦è¯­æ³•å®Œæ•´"""
    doc = nlp(sentence)
    
    # å¦‚æœå¥å­å¤ªçŸ­æˆ–è€…ä¸æ˜¯å®Œæ•´çš„ä¸»è°“ç»“æ„ï¼Œåˆ™å‰”é™¤
    if len(doc) < 5 or not any(token.dep_ == "ROOT" for token in doc):
        return False
    
    # è®¡ç®—å•è¯é¢‘ç‡ï¼Œé¿å…æ— æ„ä¹‰é‡å¤
    word_freq = Counter(word.text.lower() for word in doc if word.is_alpha)
    if max(word_freq.values()) > len(doc) * 0.5:  # å¦‚æœæŸä¸ªå•è¯å æ¯”è¶…è¿‡ 50%ï¼Œå¯èƒ½æ˜¯æ— æ„ä¹‰çš„å¥å­
        return False

    return True

filtered_sentences = [sentence for sentence in generated_sentences if is_valid_sentence(sentence)]

# ç¡®ä¿è‡³å°‘æœ‰ 3 ä¸ªå€™é€‰å¥å­å¯ç”¨
if len(filtered_sentences) < 3:
    print("âš ï¸ è¯­æ³•æ£€æµ‹åå‰©ä½™çš„å¥å­æ•°é‡è¿‡å°‘ï¼Œè¯·è°ƒæ•´ prompt æˆ–é™ä½ç­›é€‰æ ‡å‡†ã€‚")
    print(generated_sentences)
    exit()

# ================= é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ 3 ä¸ªå¥å­ =================
def sentence_similarity(s1, s2):
    """è®¡ç®—ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, s1, s2).ratio()

# æŒ‰ä¸åŸå¥çš„ç›¸ä¼¼åº¦æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„ 3 ä¸ªå¥å­
filtered_sentences = sorted(filtered_sentences, key=lambda x: sentence_similarity(input_text, x), reverse=True)[:3]

# ================= è¾“å‡ºæœ€ç»ˆç»“æœ =================
print("ğŸ”¹ åŸæ–‡ï¼š", input_text)
print("\nâœ… ç”Ÿæˆçš„æœ€ä½³ 3 ä¸ªé‡ç»„ç‰ˆæœ¬ï¼š")
for i, sentence in enumerate(filtered_sentences, 1):
    print(f"{i}. {sentence}")
