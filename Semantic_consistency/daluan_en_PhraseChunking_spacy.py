import spacy
import random

# åŠ è½½ SpaCy è‹±è¯­æ¨¡å‹
nlp = spacy.load("en_core_web_sm")

def get_chunks(sentence):
    """
    è§£æå¥å­å¹¶æå–çŸ­è¯­ï¼ˆåè¯çŸ­è¯­ã€åŠ¨è¯çŸ­è¯­ï¼‰ï¼Œé¿å…é‡å¤
    """
    doc = nlp(sentence)
    
    # è·å–åè¯çŸ­è¯­ï¼ˆNPï¼‰ï¼Œé™åˆ¶æœ€å¤§é•¿åº¦ï¼Œé¿å…æ•´å¥è¢«å½“æˆçŸ­è¯­
    noun_chunks = []
    for chunk in doc.noun_chunks:
        if len(chunk.text.split()) <= 7:  # é™åˆ¶çŸ­è¯­é•¿åº¦ï¼Œé¿å…è¿‡é•¿çŸ­è¯­
            noun_chunks.append(chunk.text)
    
    # è·å–åŠ¨è¯çŸ­è¯­ï¼ˆVPï¼‰ï¼Œç¡®ä¿æå–åˆç†é•¿åº¦
    verb_chunks = []
    for token in doc:
        if token.pos_ == "VERB":
            verb_phrase = " ".join([child.text for child in token.subtree])
            if len(verb_phrase.split()) <= 7:  # é™åˆ¶çŸ­è¯­é•¿åº¦
                verb_chunks.append(verb_phrase)

    # è®¡ç®—å‰©ä½™å•è¯ï¼ˆå»é™¤å·²åœ¨çŸ­è¯­ä¸­çš„å•è¯ï¼‰
    all_chunks_words = set(word for phrase in noun_chunks + verb_chunks for word in phrase.split())
    remaining_words = [token.text for token in doc if token.text not in all_chunks_words]

    return noun_chunks, verb_chunks, remaining_words

def scramble_sentence(sentence):
    """
    å¯¹å¥å­è¿›è¡Œæ‰“ä¹±ï¼Œä½†ä¿ç•™çŸ­è¯­å†…éƒ¨é¡ºåº
    """
    noun_chunks, verb_chunks, remaining_words = get_chunks(sentence)

    # ä»…å­˜å‚¨å”¯ä¸€çŸ­è¯­ï¼Œé¿å…é‡å¤
    all_elements = list(set(noun_chunks + verb_chunks + remaining_words))

    # ä»…æ‰“ä¹±çŸ­è¯­ & è¯åºï¼Œä¸é‡å¤æ‰§è¡Œ
    random.shuffle(all_elements)

    # é‡æ–°ç»„åˆæˆæ‰“ä¹±åçš„å¥å­
    scrambled_sentence = " ".join(all_elements)

    return scrambled_sentence

# ç¤ºä¾‹å¥å­
sentence = "To solve the problems of the data reliability for NAND flash storages, a variable-node-based belief-propagation with message pre-processing (VNBP-MP) decoding algorithm for binary low-density parity-check (LDPC) codes is proposed."

# ç”Ÿæˆæ‰“ä¹±çš„å¥å­
scrambled = scramble_sentence(sentence)

# è¾“å‡ºç»“æœ
print("ğŸ”¹ åŸå§‹å¥å­ï¼š", sentence)
print("âœ… æ‰“ä¹±åå¥å­ï¼š", scrambled)
