# modules/training/t5_base_group_training.py
# åˆ†ç»„è®­ç»ƒ T5 æ¨¡å‹ï¼šæ¯ä¸ª group_id å•ç‹¬æå–å¹¶è®­ç»ƒï¼Œä¿å­˜æ¨¡å‹ä¸ loss æ›²çº¿å›¾

import os
import json
import matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import Trainer, TrainingArguments, T5Tokenizer, T5ForConditionalGeneration
from datasets import Dataset

from modules.utils.jsonl_handler import read_jsonl
from modules.models.manager import ModelManager

# ã€å‚æ•°è®¾ç½®ã€‘
model_name = "t5-base"  # ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹

# è¾“å…¥æ•°æ®æ–‡ä»¶ï¼Œå†…å®¹åº”åŒ…å« grouped shuffle çš„æ¯æ¡å…ƒç´ ï¼ˆå« group_id ä¿¡æ¯ï¼‰
dataset_path = "data/train_pairs/grouped_shuffle_all.jsonl"

output_root = "models/grouped"  # æ¨¡å‹è¾“å‡ºæ ¹ç›®å½•
version_prefix = "t5-base_group"  # æ¨¡å‹ä¿å­˜æ–‡ä»¶å‰ç¼€

target_groups = [0, 1]  # æŒ‡å®šè¦è®­ç»ƒçš„ group_id 
num_epochs = 3  # è®­ç»ƒè½®æ•°

# ã€åŠ è½½ Tokenizer å’Œæ¨¡å‹ï¼Œä½¿ç”¨ ModelManager é˜²æ­¢é‡å¤è½½å…¥ã€‘
if ModelManager.is_model_loaded(model_name):
    model, tokenizer = ModelManager.get_model(model_name)
else:
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    ModelManager.register_model(model_name, model, tokenizer)

# ã€åˆ†ç»„æ•°æ®åŠ è½½ + å¤„ç†ã€‘
def load_group_data(group_id, all_data):
    """æ ¹æ® group_id è¿‡æ»¤æ•°æ®ï¼Œå¹¶è¿›è¡Œåˆ†ç±»è¾“å…¥å’Œæ ‡ç­¾ç¼–ç ã€‚è¿”å› train/test åˆ†å‰²
    """
    group_data = [item for item in all_data if item.get("metadata", {}).get("group_id") == group_id]
    dataset = Dataset.from_list(group_data)

    def preprocess_function(examples):
        inputs = ["reorder: " + s for s in examples["shuffled"]]  # prompt prefix
        targets = examples["original"]
        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
        labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    dataset = dataset.map(preprocess_function, batched=True)
    return dataset.train_test_split(test_size=0.2, seed=42)["train"], dataset.train_test_split(test_size=0.2, seed=42)["test"]

# ã€ä¿å­˜ loss æ›²çº¿å›¾ã€‘
def plot_and_save_loss(logs, save_path):
    steps = list(range(1, len(logs) + 1))
    losses = [entry["loss"] for entry in logs]
    plt.figure(figsize=(8, 5))
    plt.plot(steps, losses, label="Train Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

# ã€è®­ç»ƒå•ä¸ª group_id æ¨¡å‹ã€‘
def train_model_on_group(group_id, all_data):
    print(f"\nğŸš€ æ­£åœ¨è®­ç»ƒ Group {group_id}...")
    train_dataset, eval_dataset = load_group_data(group_id, all_data)

    group_tag = f"{version_prefix}_{group_id}"
    output_dir = os.path.join(output_root, group_tag)
    os.makedirs(output_dir, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        fp16=True,
        num_train_epochs=num_epochs,
        save_total_limit=3,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_steps=100,
        eval_steps=500,
        report_to="none",
        dataloader_pin_memory=True,
        dataloader_num_workers=1,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # âœ… è®°å½• loss
    loss_log = []
    def log_callback(info):
        if "loss" in info:
            loss_log.append({"step": info.get("step", len(loss_log)), "loss": info["loss"]})

    trainer.add_callback(
        type("LossLogger", (), {
            "on_log": lambda self, args, state, control, logs=None, **kwargs: log_callback(logs or {})
        })()
    )

    trainer.train()

    # âœ… ä¿å­˜æ¨¡å‹
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {output_dir}")

    # âœ… ä¿å­˜ loss å›¾
    plot_path = os.path.join(output_dir, "loss_curve.png")
    plot_and_save_loss(loss_log, plot_path)
    print(f"ğŸ“ˆ Loss å›¾å·²ä¿å­˜è‡³ {plot_path}")

    # âœ… ä¿å­˜è¯„ä¼°ç»“æœ
    eval_result = trainer.evaluate()
    eval_path = os.path.join(output_dir, "eval_results.json")
    with open(eval_path, "w") as f:
        json.dump(eval_result, f, indent=4)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ {eval_path}")

# ã€æ­£å¼æ‰§è¡Œè®­ç»ƒã€‘
if __name__ == "__main__":
    all_data = read_jsonl(dataset_path)
    for gid in target_groups:
        train_model_on_group(gid, all_data)