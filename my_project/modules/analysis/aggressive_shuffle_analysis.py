# modules/analysis/aggressive_shuffle_analysis.py
# ç”¨äºå®éªŒæ€§åœ°ç”Ÿæˆå¥å­æ‰“ä¹±ç‰ˆæœ¬å¹¶è¯„ä¼°å…¶è¯­ä¹‰ä¸ç»“æ„å˜åŒ–

import random
import numpy as np
from collections import Counter
from nltk import ngrams
from nltk.metrics import edit_distance
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer, util
import torch

# âœ… åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯¹è¯åºæ›´æ•æ„Ÿçš„æ¨¡å‹ï¼‰
model = SentenceTransformer("hkunlp/instructor-large", device="cuda" if torch.cuda.is_available() else "cpu")

# âœ… æ”¹å†™åçš„æ‰“ä¹±å‡½æ•°ï¼šä¸å†ä½¿ç”¨ [MASK] æ›¿æ¢

def aggressive_shuffle(text, p_char=0.2):
    """
    æ··åˆæ‰“ä¹±ç­–ç•¥ï¼ˆæ—  maskï¼‰ï¼š
    - éšæœºæ‰“ä¹±å•è¯é¡ºåº
    - æŒ‰æ¦‚ç‡ p_char æ‰“ä¹±è¯å†…éƒ¨å­—ç¬¦ï¼ˆé•¿åº¦>3ï¼‰
    """
    words = text.split()

    for i in range(len(words)):
        if random.random() < p_char and len(words[i]) > 3:
            words[i] = ''.join(random.sample(words[i], len(words[i])))

    random.shuffle(words)
    return ' '.join(words)


def evaluate_dissimilarity(original, shuffled):
    """è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„ç›¸ä¼¼åº¦/å·®å¼‚åº¦ï¼šè¯­ä¹‰ã€bigramã€ç¼–è¾‘è·ç¦»"""
    # åµŒå…¥è¯­ä¹‰ç›¸ä¼¼åº¦
    emb_orig = model.encode(original, convert_to_tensor=True)
    emb_shuf = model.encode(shuffled, convert_to_tensor=True)
    semantic_sim = util.pytorch_cos_sim(emb_orig, emb_shuf).item()

    # bigram ç›¸ä¼¼åº¦
    def ngram_overlap(a, b, n=2):
        a_ngrams = Counter(ngrams(a.split(), n))
        b_ngrams = Counter(ngrams(b.split(), n))
        intersection = sum((a_ngrams & b_ngrams).values())
        return intersection / max(len(a_ngrams), 1)

    bigram_sim = ngram_overlap(original, shuffled)

    # ç¼–è¾‘è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰
    def norm_edit_distance(a, b):
        m, n = len(a.split()), len(b.split())
        max_len = max(m, n)
        return edit_distance(a.split(), b.split()) / max_len if max_len > 0 else 0

    # ç»„åˆå·®å¼‚åˆ†æ•°ï¼ˆè¶Šå¤§è¶Šä¸ç›¸ä¼¼ï¼‰
    return {
        "semantic_similarity": semantic_sim,
        "bigram_similarity": bigram_sim,
        "combined_score": 0.6 * (1 - semantic_sim) + 0.3 * (1 - bigram_sim) + 0.1 * norm_edit_distance(original, shuffled)
    }


def run_experiment(text, n_trials=200):
    results = []
    min_score = float('inf')
    best_version = None

    for _ in range(n_trials):
        shuffled = aggressive_shuffle(text)
        scores = evaluate_dissimilarity(text, shuffled)
        results.append(scores)

        if scores["combined_score"] < min_score:
            min_score = scores["combined_score"]
            best_version = shuffled

    # å¯è§†åŒ–åˆ†å¸ƒ
    fig, ax = plt.subplots(1, 3, figsize=(15, 4))
    ax[0].hist([r["semantic_similarity"] for r in results], bins=20)
    ax[0].set_title("Semantic Similarity")

    ax[1].hist([r["bigram_similarity"] for r in results], bins=20)
    ax[1].set_title("Bigram Overlap")

    ax[2].hist([r["combined_score"] for r in results], bins=20)
    ax[2].set_title("Combined Dissimilarity")

    plt.tight_layout()
    plt.show()

    return best_version, min_score, results


if __name__ == "__main__":
    original_text = "In this paper, we propose a simple yet effective black-box zero-shot detection approach based on the observation that, from the perspective of LLMs, human-written texts typically contain more grammatical errors than LLM-generated texts."

    best_shuffled, min_score, results = run_experiment(original_text)

    print("\nâœ… Original:", original_text)
    print("ğŸ”€ Best Shuffled:", best_shuffled)
    print(f"ğŸ“‰ Min Combined Score: {min_score:.4f}")

    # ç®€è¦ç»Ÿè®¡
    avg_sem = np.mean([r["semantic_similarity"] for r in results])
    avg_bi = np.mean([r["bigram_similarity"] for r in results])
    print("\nğŸ“Š Average Similarity (200 Trials):")
    print(f"Semantic: {avg_sem:.4f}, Bigram: {avg_bi:.4f}")