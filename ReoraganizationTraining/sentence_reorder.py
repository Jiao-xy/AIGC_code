import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# **1ï¸âƒ£ é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹**
models = {
    "T5-Base": "t5-base",
    "BART-Large": "facebook/bart-large",
    "UL2": "google/ul2"
}

# **2ï¸âƒ£ éœ€è¦æµ‹è¯•çš„ä¹±åºå¥å­**
sentences = [
    "NAND solve codes , the of data reliability for To flash algorithm problems a variable-node-based belief-propagation with message pre-processing ( VNBP-MP ) decoding storages for binary LDPC parity-check ( low-density ) the is proposed .",
    "The major that is effectively , . making use of the pre-processing of the NAND feature channel , propagation proposed algorithm performs the message characteristics ( MP ) scheme reliable flash prevent the propagation of unreliable the and speed up messages by of to messages the",
    "To ) speed up the decoding convergence , the further for oscillating variable nodes ( treatment VNs being considered after . MP scheme is employed the",
    "noticeable has show that . proposed VNBP-MP algorithm improvement a Simulation results in convergence speed without compromising the error-correction performance , compared with the existing algorithms the",
    "To solve , simultaneous one and mapping ( SLAM ) problem , many have PF been proposed effective and the Particle Filter ( techniques ) is localization of ways the .",
    "However , the PF of approximate a large number algorithm samples to needs the posterior probability . of the system , which the makes algorithm complex density",
        
]

# **3ï¸âƒ£ æ¨ç†å‡½æ•°**
def reorder_sentence(model, tokenizer, sentence):
    input_text = "reorder: " + sentence  # **T5 é£æ ¼è¾“å…¥**
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    output_ids = model.generate(
        input_ids,
        max_length=128,
        do_sample=True,  # é‡‡æ ·ç”Ÿæˆ
        top_k=50,  # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ 50 ä¸ª token
        top_p=0.95,  # ä»…ä¿ç•™ç´¯ç§¯æ¦‚ç‡ 95% çš„ token
        temperature=0.7,  # æ§åˆ¶é‡‡æ ·çš„éšæœºæ€§
    )

    reordered_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return reordered_sentence

# **4ï¸âƒ£ æŒ‰æ¨¡å‹é€ä¸ªåŠ è½½ï¼Œå‡å°‘æ˜¾å­˜å ç”¨**
for model_name, model_path in models.items():
    print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_name} ...")
    
    # **åªåŠ è½½å½“å‰æ¨¡å‹**
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"âœ… {model_name} åŠ è½½å®Œæˆï¼Œå¼€å§‹æ¨ç†...")
    
    # **æ¨ç†å¥å­**
    for shuffled_sentence in tqdm(sentences, desc=f"â³ å¤„ç† {model_name}", unit="å¥"):
        reordered = reorder_sentence(model, tokenizer, shuffled_sentence)
        print(f"ğŸ”„ ä¹±åºå¥å­: {shuffled_sentence}")
        print(f"âœ… {model_name} é‡ç»„å¥å­: {reordered}\n")
    
    # **é‡Šæ”¾æ˜¾å­˜ï¼Œå‡å°‘å ç”¨**
    del model
    torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜

print("ğŸ‰ æ‰€æœ‰æ¨¡å‹æ¨ç†å®Œæˆï¼")

