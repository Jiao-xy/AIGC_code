import json
import pandas as pd
import random
import nltk
from tqdm import tqdm
from scipy.stats import kendalltau, spearmanr
nltk.download("punkt")
from nltk.tokenize import sent_tokenize, word_tokenize

# **1ï¸âƒ£ è¯»å– JSONL æ–‡ä»¶**
data_files = [
    "/home/jxy/Data/init/ieee-init.jsonl",
    "/home/jxy/Data/init/ieee-chatgpt-generation.jsonl"
]
data_pairs = []

#ç”Ÿæˆä¸åŒ Kendallâ€™s Tau çº§åˆ«çš„æ‰“ä¹±å¥å­
def shuffle_with_tau(sentence, tau=0.5):
    words = word_tokenize(sentence)  # è¯åˆ†å‰²
    num_swaps = int(len(words) * (1 - tau))  # æ§åˆ¶æ‰“ä¹±ç¨‹åº¦
    shuffled = words[:]
    
    for _ in range(num_swaps):
        i, j = random.sample(range(len(words)), 2)
        shuffled[i], shuffled[j] = shuffled[j], shuffled[i]

    return " ".join(shuffled)

#è®¡ç®— Kendallâ€™s Tauã€Spearmanâ€™s ç›¸å…³æ€§ã€Position Distance
def compute_metrics(original, shuffled):
    original_order = {word: i for i, word in enumerate(original.split())}
    shuffled_order = [original_order[word] for word in shuffled.split() if word in original_order]

    if len(shuffled_order) < 2:
        return 0, 0, 0  # ä¸èƒ½è®¡ç®— Kendallâ€™s Tau æˆ– Spearmanâ€™s ç›¸å…³æ€§

    tau_score, _ = kendalltau(list(range(len(shuffled_order))), shuffled_order)
    spearman_score, _ = spearmanr(list(range(len(shuffled_order))), shuffled_order)
    
    # è®¡ç®— Position Distance
    position_distances = [abs(i - shuffled_order[i]) for i in range(len(shuffled_order))]
    pos_dist_score = sum(position_distances) / len(position_distances)

    return tau_score, spearman_score, pos_dist_score


# **ç»Ÿè®¡æ€»è¡Œæ•°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰**
total_lines = sum(1 for file in data_files for _ in open(file, "r", encoding="utf-8"))
print(f"ğŸ“„ å‘ç° {total_lines} æ¡æ‘˜è¦æ•°æ®ï¼Œå¼€å§‹å¤„ç†...\n")

# **2ï¸âƒ£ è§£æ JSONL æ–‡ä»¶**
for file in data_files:
    with open(file, "r", encoding="utf-8") as f:
        for line in tqdm(f, total=total_lines, desc="â³ è¯»å– JSONL æ•°æ®", unit="æ¡æ‘˜è¦"):
            try:
                entry = json.loads(line.strip())  
                abstract_text = entry.get("abstract", "").strip()

                if abstract_text:
                    sentences = sent_tokenize(abstract_text)  

                    for sentence in sentences:  # âœ… ç›´æ¥éå†å¥å­ï¼Œä¸ä½¿ç”¨ tqdm
                        # ç”Ÿæˆä¸åŒæ‰“ä¹±çº§åˆ«çš„å¥å­
                        shuffled_08 = shuffle_with_tau(sentence, tau=0.8)
                        shuffled_05 = shuffle_with_tau(sentence, tau=0.5)
                        shuffled_02 = shuffle_with_tau(sentence, tau=0.2)

                        # è®¡ç®—æ‰“ä¹±æŒ‡æ ‡
                        tau_08, spearman_08, pos_dist_08 = compute_metrics(sentence, shuffled_08)
                        tau_05, spearman_05, pos_dist_05 = compute_metrics(sentence, shuffled_05)
                        tau_02, spearman_02, pos_dist_02 = compute_metrics(sentence, shuffled_02)

                        # **6ï¸âƒ£ æ·»åŠ åˆ°æ•°æ®é›†**
                        data_pairs.append({
                            "åŸå¥": sentence,
                            "æ‰“ä¹±å¥å­_08": shuffled_08, "tau_08": tau_08, "spearman_08": spearman_08, "pos_dist_08": pos_dist_08,
                            "æ‰“ä¹±å¥å­_05": shuffled_05, "tau_05": tau_05, "spearman_05": spearman_05, "pos_dist_05": pos_dist_05,
                            "æ‰“ä¹±å¥å­_02": shuffled_02, "tau_02": tau_02, "spearman_02": spearman_02, "pos_dist_02": pos_dist_02,
                        })

            except json.JSONDecodeError:
                print(f"âŒ JSON è§£æå¤±è´¥: {line}")

# **7ï¸âƒ£ ä¿å­˜æ•°æ®ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰**
output_file = "sentence_shuffled_dataset.csv"
df = pd.DataFrame(data_pairs)

print("\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")
for _ in tqdm(range(100), desc="ğŸ’¾ å†™å…¥ CSV æ–‡ä»¶"):
    df.to_csv(output_file, index=False, encoding="utf-8")

print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜è‡³: {output_file}")
