import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset
import os
from tqdm import tqdm
import json

# **1ï¸âƒ£ è®¾å®šæ¨¡å‹åŠæ•°æ®é›†å‚æ•°**
model_names = {"t5-small": "t5-small", "t5-base": "t5-base"}
dataset_files = {
    "tau_08": "/home/jxy/Data/ReoraganizationData/sentence_shuffled_dataset_tau_08.jsonl",
    "reorder": "/home/jxy/Data/ReoraganizationData/sentence_reorder_dataset.jsonl",
}
output_dir = "/home/jxy/models"

# **2ï¸âƒ£ é¢„åŠ è½½ Tokenizer**
tokenizer = T5Tokenizer.from_pretrained("t5-small")  # é¢„åŠ è½½ tokenizer
processed_datasets = {}

# **3ï¸âƒ£ è¯»å–å¹¶é¢„å¤„ç†æ•°æ®**
for dataset_key, dataset_path in dataset_files.items():
    print(f"\nğŸ“¥ åŠ è½½æ•°æ®é›†: {dataset_path}")

    # **è¯»å– JSONL æ•°æ®**
    def load_jsonl(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return [json.loads(line.strip()) for line in f.readlines()]

    data = load_jsonl(dataset_path)

    # **è½¬æ¢ä¸º Hugging Face Dataset**
    dataset = Dataset.from_list(data)

    # **é¢„å¤„ç†å‡½æ•°**
    def preprocess_function(examples):
        inputs = ["reorder: " + s for s in examples["shuffled_sentence"]]
        targets = examples["original_sentence"]
        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
        labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    # **åº”ç”¨é¢„å¤„ç†**
    dataset = dataset.map(preprocess_function, batched=True)

    # **æ‹†åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†**
    dataset = dataset.train_test_split(test_size=0.2, seed=42)
    
    processed_datasets[dataset_key] = dataset
    print(f"âœ… æ•°æ®é›† `{dataset_key}` å¤„ç†å®Œæˆï¼ŒåŒ…å« {len(dataset['train'])} æ¡è®­ç»ƒæ•°æ® å’Œ {len(dataset['test'])} æ¡éªŒè¯æ•°æ®")

# **4ï¸âƒ£ è®­ç»ƒå¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹**
def train_model(model_name, dataset_key, version_name):
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name} - {version_name}...")

    # å–å·²å¤„ç†å¥½çš„æ•°æ®
    train_dataset = processed_datasets[dataset_key]["train"]
    eval_dataset = processed_datasets[dataset_key]["test"]

    # åŠ è½½æ¨¡å‹
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    # è®­ç»ƒå‚æ•°
    if model_name == "t5-small":
        training_args = TrainingArguments(
            output_dir=os.path.join(output_dir, version_name),
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            gradient_accumulation_steps=1,
            fp16=True,
            num_train_epochs=5,
            save_total_limit=3,
            eval_strategy="epoch",
            save_strategy="epoch",
            logging_steps=500,
            eval_steps=2000,
            report_to="none",
            dataloader_pin_memory=True,
            dataloader_num_workers=1,
        )
    else:  # T5-Base
        training_args = TrainingArguments(
            output_dir=os.path.join(output_dir, version_name),
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=4,
            fp16=True,
            num_train_epochs=5,
            save_total_limit=3,
            eval_strategy="epoch",
            save_strategy="epoch",
            logging_steps=500,
            eval_steps=2000,
            report_to="none",
            dataloader_pin_memory=True,
            dataloader_num_workers=1,
        )
    
    # è®­ç»ƒ
    torch.backends.cudnn.benchmark = True
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )
    
    epochs = training_args.num_train_epochs
    eval_results = {}

    for epoch in tqdm(range(epochs), desc=f"Training {version_name}"):
        trainer.train()

        # åœ¨ 30%ã€50%ã€80% è¿›åº¦ç‚¹è¿›è¡Œè¯„ä¼°å¹¶ä¿å­˜æ¨¡å‹
        for progress, tag in zip([0.3, 0.5, 0.8], ["30", "50", "80"]):
            if epoch == int(epochs * progress) - 1:
                save_path = os.path.join(output_dir, f"{version_name}_{tag}")
                model.save_pretrained(save_path)
                tokenizer.save_pretrained(save_path)
                print(f"âœ… {tag}% è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")
                eval_metrics = trainer.evaluate()
                eval_results[tag] = eval_metrics
                print(f"ğŸ“Š {tag}% è¯„ä¼°ç»“æœ: {eval_metrics}")

    # 100% è®­ç»ƒå®Œæˆåè¯„ä¼°å¹¶ä¿å­˜
    save_path_final = os.path.join(output_dir, f"{version_name}_100")
    model.save_pretrained(save_path_final)
    tokenizer.save_pretrained(save_path_final)
    eval_metrics = trainer.evaluate()
    eval_results["100"] = eval_metrics
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œå®Œæ•´æ¨¡å‹å·²ä¿å­˜è‡³ {save_path_final}ï¼")
    print(f"ğŸ“Š 100% è¯„ä¼°ç»“æœ: {eval_metrics}")

    # ä¿å­˜è¯„ä¼°ç»“æœåˆ° JSON æ–‡ä»¶
    eval_results_path = os.path.join(output_dir, f"{version_name}_eval_results.json")
    with open(eval_results_path, "w") as f:
        json.dump(eval_results, f, indent=4)
    print(f"ğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ {eval_results_path}")

# **5ï¸âƒ£ è®­ç»ƒæ‰€æœ‰ç‰ˆæœ¬çš„æ¨¡å‹**
for model_key, model_name in model_names.items():
    for dataset_key in dataset_files.keys():
        version_name = f"{model_key}_{dataset_key}"
        train_model(model_name, dataset_key, version_name)
