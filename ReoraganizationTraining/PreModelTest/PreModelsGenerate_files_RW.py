import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
import os

# **1ï¸âƒ£ è®¾ç½®æ¨¡å‹ç›®å½•**
models_dir = "/home/jxy/models"  # è®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„
input_file = "input_sentences.txt"  # è¾“å…¥æ–‡ä»¶
output_file = "output_results.txt"  # è¾“å‡ºæ–‡ä»¶

# **2ï¸âƒ£ è¯»å–æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹**
def get_trained_models(models_dir):
    model_versions = []
    for model_name in os.listdir(models_dir):
        model_path = os.path.join(models_dir, model_name)
        if os.path.isdir(model_path) and (model_name.startswith("t5-small") or model_name.startswith("t5-base")):
            model_versions.append(model_name)
    return sorted(model_versions)

# **3ï¸âƒ£ ä»æ–‡ä»¶ä¸­è¯»å–éœ€è¦é‡ç»„çš„å¥å­**
def read_sentences_from_file(file_path):
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼")
        return []
    with open(file_path, "r", encoding="utf-8") as file:
        sentences = [line.strip() for line in file.readlines() if line.strip()]
    return sentences

# **4ï¸âƒ£ å¥å­é‡ç»„å‡½æ•°ï¼ˆæ”¯æŒä¸‰ç§æ¨¡å¼ï¼‰**
def reorder_sentence(model, tokenizer, sentence, mode="beam_search"):
    input_text = f"reorder: {sentence}"
    inputs_encodings = tokenizer(
        input_text, return_tensors="pt", truncation=True, max_length=128
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        if mode == "beam_search":
            output_ids = model.generate(
                input_ids=inputs_encodings.input_ids,
                attention_mask=inputs_encodings.attention_mask,
                max_length=128,
                num_beams=8,
                repetition_penalty=1.5,
                length_penalty=0.8,
                early_stopping=True,
            )
        elif mode == "sampling":
            output_ids = model.generate(
                input_ids=inputs_encodings.input_ids,
                attention_mask=inputs_encodings.attention_mask,
                max_length=128,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.95,
            )
        elif mode == "beam_sampling":
            output_ids = model.generate(
                input_ids=inputs_encodings.input_ids,
                attention_mask=inputs_encodings.attention_mask,
                max_length=128,
                num_beams=5,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.5,
                length_penalty=0.8,
            )
        else:
            raise ValueError("Invalid mode. Choose from 'beam_search', 'sampling', or 'beam_sampling'")

    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# **5ï¸âƒ£ éå†å¤šä¸ªæ¨¡å‹å¹¶é‡ç»„å¥å­**
device = "cuda" if torch.cuda.is_available() else "cpu"
model_versions = get_trained_models(models_dir)
test_sentences = read_sentences_from_file(input_file)

if not test_sentences:
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥å¥å­ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
else:
    with open(output_file, "w", encoding="utf-8") as out_f:
        out_f.write(f"ğŸ“Œ å…±æ‰¾åˆ° {len(model_versions)} ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹:\n")
        for model_version in model_versions:
            out_f.write(f" - {model_version}\n")
        
        modes = ["beam_search", "sampling", "beam_sampling"]
        
        for model_version in model_versions:
            model_path = os.path.join(models_dir, model_version)
            if not os.path.exists(model_path):
                out_f.write(f"âš ï¸ æ¨¡å‹ {model_version} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ï¼\n")
                continue
            
            print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_version}")
            tokenizer = T5Tokenizer.from_pretrained(model_path)
            model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)
            
            out_f.write(f"\nğŸ”¹ æ¨¡å‹: {model_version}\n")
            for mode in modes:
                out_f.write(f"\nğŸ”¹ ç”Ÿæˆæ¨¡å¼: {mode}\n")
                for i, original in enumerate(test_sentences):
                    reordered = reorder_sentence(model, tokenizer, original, mode=mode)
                    out_f.write(f"ã€å¥å­ {i+1}ã€‘âœ… é‡ç»„å: {reordered}\n")
            
            del model
            torch.cuda.empty_cache()

    print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {output_file}")
