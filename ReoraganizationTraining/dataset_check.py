import pandas as pd
import csv

def load_and_clean_dataset(file_path, expected_columns, max_length=512):
    """ è¯»å–å¹¶æ¸…ç†æ•°æ®é›†ï¼Œå¹¶ç»Ÿè®¡è¿‡æ»¤çš„æ•°æ®é‡ """
    print(f"\nğŸ“¥ åŠ è½½æ•°æ®é›†: {file_path}")
    
    try:
        # è¯»å– CSVï¼Œå¹¶è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ
        df = pd.read_csv(file_path, on_bad_lines="skip", quoting=csv.QUOTE_NONE)
        total_rows = len(df)  # è¯»å–åæ€»è¡Œæ•°
        
        # 1ï¸âƒ£ æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„åˆ—
        missing_cols = [col for col in expected_columns if col not in df.columns]
        if missing_cols:
            print(f"âš ï¸ è­¦å‘Šï¼šæ•°æ®é›†ç¼ºå°‘åˆ— {missing_cols}ï¼Œè·³è¿‡è¯¥æ•°æ®é›†ã€‚")
            return None

        # 2ï¸âƒ£ åˆ é™¤ NaN å€¼
        before_drop = len(df)
        df = df.dropna()
        after_drop = len(df)
        dropped_nan = before_drop - after_drop
        
        # 3ï¸âƒ£ è¿‡æ»¤è¶…é•¿æ–‡æœ¬ï¼ˆ> 512ï¼‰
        before_filter = len(df)
        df["input_length"] = df[expected_columns[0]].astype(str).apply(len)
        df["target_length"] = df[expected_columns[1]].astype(str).apply(len)
        df = df[(df["input_length"] <= max_length) & (df["target_length"] <= max_length)]
        after_filter = len(df)
        dropped_long_text = before_filter - after_filter
        
        # ç»Ÿè®¡è¿‡æ»¤æ¯”ä¾‹
        print(f"ğŸ”¹ æ€»æ•°æ®é‡: {total_rows}")
        print(f"âš ï¸ è¿‡æ»¤ NaN æ•°æ®: {dropped_nan} è¡Œ")
        print(f"âš ï¸ è¿‡æ»¤è¶…é•¿æ–‡æœ¬ (>512 tokens): {dropped_long_text} è¡Œ")
        print(f"âœ… è¿‡æ»¤åå‰©ä½™æ•°æ®: {after_filter} è¡Œ")
        
        return df  # è¿”å›æ¸…ç†åçš„æ•°æ®é›†

    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®é›† {file_path} å¤±è´¥: {e}")
        return None


# **è¿è¡Œæ•°æ®é›†æ£€æŸ¥**
dataset_files = {
    "tau_08": "/home/jxy/Data/ReoraganizationData/sentence_shuffled_dataset_tau_08.csv",
    "full": "/home/jxy/Data/ReoraganizationData/sentence_reorder_dataset.csv"
}

expected_columns = ["ä¹±åºå¥å­", "æ­£ç¡®å¥å­"]

filtered_datasets = {}

for name, path in dataset_files.items():
    print(f"\nğŸ” æ­£åœ¨æ£€æŸ¥æ•°æ®é›†ï¼š{name}")
    filtered_datasets[name] = load_and_clean_dataset(path, expected_columns)

print("\nğŸ“Œ æ•°æ®é›†æ£€æµ‹å®Œæˆï¼Œè‹¥æ— é”™è¯¯ï¼Œåˆ™å¯ç”¨äºè®­ç»ƒï¼")
