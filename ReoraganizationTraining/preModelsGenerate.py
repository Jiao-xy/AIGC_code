import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
import os

# **1ï¸âƒ£ è®¾ç½®æ¨¡å‹ç›®å½•**
models_dir = "/home/jxy/models"  # è®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„

# **2ï¸âƒ£ è¯»å–æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹**
def get_trained_models(models_dir):
    """
    è¯»å– /home/jxy/models/ ç›®å½•ï¼Œè·å–æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹å­ç›®å½•
    - ä»…ä¿ç•™ä»¥ t5-small_* æˆ– t5-base_* å¼€å¤´çš„æ¨¡å‹
    - å¿½ç•¥ JSON è¯„ä¼°ç»“æœæ–‡ä»¶
    - è‹¥å­˜åœ¨ `t5-small_tau_08/` è¿™æ ·çš„ç›®å½•ï¼Œä¼˜å…ˆåŠ è½½ `_100` ç‰ˆæœ¬
    """
    model_versions = []
    
    for model_name in os.listdir(models_dir):
        model_path = os.path.join(models_dir, model_name)
        
        # åªé€‰å– T5 ç›¸å…³çš„æ¨¡å‹ç›®å½•ï¼Œæ’é™¤è¯„ä¼°æ–‡ä»¶
        if os.path.isdir(model_path) and (model_name.startswith("t5-small") or model_name.startswith("t5-base")):
            model_versions.append(model_name)

    # å¤„ç† `_100` ç‰ˆæœ¬ä¼˜å…ˆ
    final_model_list = []
    for model_name in model_versions:
        base_name = model_name.rstrip("_100").rstrip("_30").rstrip("_50").rstrip("_80")
        if base_name in model_versions and f"{base_name}_100" in model_versions:
            continue  # å¦‚æœ `_100` ç‰ˆæœ¬å­˜åœ¨ï¼Œåˆ™è·³è¿‡é `_100` ç‰ˆæœ¬
        final_model_list.append(model_name)
    
    return sorted(final_model_list)  # æ’åºï¼Œä¿è¯åŠ è½½é¡ºåºä¸€è‡´

# **3ï¸âƒ£ è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹**
model_versions = get_trained_models(models_dir)

# **4ï¸âƒ£ éœ€è¦é‡ç»„çš„å¥å­**
test_sentences = shuffled_sentences = [
    "and the challenges facing the research area. by summarizing key implications, Finally, we conclude future research directions,",
    "The last decade has seen due to the unprecedented success of deep learning. in this area a surge of research",
    "datasets, and evaluation metrics have been proposed in the literature, Numerous methods, raising the need for a comprehensive and updated survey.",
    "support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey.",
    "This paper fills the gap by reviewing the state-of-the-art approaches focusing on models from traditional models to deep learning. from 1961 to 2021,",
    "natural language processing. Text classification is the most fundamental and essential task in",
    "for text classification according to the text involved and the models used for feature extraction and classification. We create a taxonomy",
    "We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that"
]

# **5ï¸âƒ£ å¥å­é‡ç»„å‡½æ•°**
def reorder_sentences(model, tokenizer, sentences):
    """
    ä½¿ç”¨æŒ‡å®šçš„ T5 æ¨¡å‹å¯¹å¤šä¸ªå¥å­è¿›è¡Œé‡ç»„
    """
    inputs = [f"reorder: {sentence}" for sentence in sentences]
    
    # **Tokenize**
    inputs_encodings = tokenizer(
        inputs, return_tensors="pt", padding=True, truncation=True, max_length=128
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    # **æ¨¡å‹é¢„æµ‹**
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs_encodings.input_ids,
            attention_mask=inputs_encodings.attention_mask,
            max_length=128,
            num_return_sequences=1,  # åªç”Ÿæˆä¸€ä¸ªå€™é€‰ç»“æœ
            temperature=0.7,  # æ§åˆ¶é‡‡æ ·å¤šæ ·æ€§
            top_k=50,  # é€‰å–æ¦‚ç‡æœ€é«˜çš„å‰50ä¸ªå•è¯
            top_p=0.95,  # è¿‡æ»¤æ‰ä½æ¦‚ç‡è¯
        )

    # **è§£ç ç”Ÿæˆç»“æœ**
    reordered_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return reordered_sentences

# **6ï¸âƒ£ éå†å¤šä¸ªæ¨¡å‹å¹¶é‡ç»„å¥å­**
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"\nğŸ“Œ å…±æ‰¾åˆ° {len(model_versions)} ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹:\n")
for model_version in model_versions:
    print(f" - {model_version}")

for model_version in model_versions:
    model_path = os.path.join(models_dir, model_version)

    if not os.path.exists(model_path):
        print(f"âš ï¸ æ¨¡å‹ {model_version} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ï¼")
        continue

    print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_version}")
    
    # **åŠ è½½ Tokenizer å’Œ Model**
    tokenizer = T5Tokenizer.from_pretrained(model_path)
    model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)

    # **æ‰§è¡Œå¥å­é‡ç»„**
    reordered_results = reorder_sentences(model, tokenizer, test_sentences)

    # **è¾“å‡ºç»“æœ**
    print(f"\nğŸ”¹ æ¨¡å‹: {model_version}")
    for i, (original, reordered) in enumerate(zip(test_sentences, reordered_results)):
        print(f"ã€å¥å­ {i+1}ã€‘")
        print(f"   ğŸ”¹ åŸå§‹å¥å­: {original}")
        print(f"   âœ… é‡ç»„å: {reordered}\n")

    # **é‡Šæ”¾æ˜¾å­˜**
    del model
    torch.cuda.empty_cache()
