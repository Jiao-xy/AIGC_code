import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
import os

# **1ï¸âƒ£ è®¾ç½®æ¨¡å‹ç›®å½•**
models_dir = "/home/jxy/models"  # è®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„

# **2ï¸âƒ£ é€‰æ‹©æ¨¡å‹**
model_name = "t5-small_tau_08_100"  # è¿™é‡Œç›´æ¥æŒ‡å®šæŸä¸ªæ¨¡å‹
model_path = os.path.join(models_dir, model_name)

if not os.path.exists(model_path):
    raise FileNotFoundError(f"âš ï¸ æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")

# **3ï¸âƒ£ éœ€è¦é‡ç»„çš„å¥å­**
test_sentences = [
    #"The rapid advancement of language large (LLM) model particularly technology, ChatGPT, the distinguishing emergence between of and texts models advanced like human-written LLM-generated has increasingly challenging. become",
    "This phenomenon unprecedented challenges presents academic authenticity, to integrity and making of detection a LLM-generated pressing research. concern in scientific",
    #"To effectively and detect accurately generated LLMs, by this constructs study a comprehensive dataset of medical paper introductions, both encompassing human-written and LLM-generated content.",
    "Based dataset, on this simple and an efficient black-box, detection zero-shot method proposed. is",
    "The method builds upon that hypothesis differences fundamental exist in linguistic logical between ordering human-written and texts. LLMgenerated",
    #"Specifically, reorders this original method text using dependency trees, parse calculates the similarity (Rscore) score between reordered the text and original, the integrates and log-likelihood as features metrics. auxiliary",
    #"The approach reordered synthesizes similarity log-likelihood and scores derive to composite a establishing metric, effective classification an for threshold discriminating between human-written and texts. LLM-generated",
    #"The results experimental our show approach that not effectively only detects but texts LLMgenerated also identifies LLM-polished abstracts, state-of-the-art outperforming current zero-shot detection methods (SOTA)."
]

# **4ï¸âƒ£ å¥å­é‡ç»„å‡½æ•°**
def reorder_sentences(model, tokenizer, sentences):
    inputs = [f"reorder: {sentence}" for sentence in sentences]
    
    inputs_encodings = tokenizer(
        inputs, return_tensors="pt", padding=True, truncation=True, max_length=128
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs_encodings.input_ids,
            attention_mask=inputs_encodings.attention_mask,
            max_length=128,
            num_return_sequences=1,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
        )

    reordered_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return reordered_sentences

# **5ï¸âƒ£ åŠ è½½æ¨¡å‹**
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_name}")
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)

# **6ï¸âƒ£ æ‰§è¡Œå¥å­é‡ç»„**
reordered_results = reorder_sentences(model, tokenizer, test_sentences)

# **7ï¸âƒ£ è¾“å‡ºç»“æœ**
print(f"\nğŸ”¹ æ¨¡å‹: {model_name}")
for i, (original, reordered) in enumerate(zip(test_sentences, reordered_results)):
    """ print(f"ã€å¥å­ {i+1}ã€‘")
    print(f"   ğŸ”¹ åŸå§‹å¥å­: {original}") """
    print(f" ã€å¥å­ {i+1}  âœ… é‡ç»„å: {reordered}\n")

# **8ï¸âƒ£ é‡Šæ”¾æ˜¾å­˜**
del model
torch.cuda.empty_cache()