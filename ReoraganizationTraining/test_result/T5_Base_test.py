from transformers import T5Tokenizer
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
import os


# **1ï¸âƒ£ è®¾ç½®æ¨¡å‹ç›®å½•**
models_dir = "/home/jxy/models"  # è®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„

def get_trained_models(models_dir):
    """
    è¯»å– /home/jxy/models/ ç›®å½•ï¼Œè·å–æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹å­ç›®å½•
    - ä»…ä¿ç•™ä»¥ t5-small_* æˆ– t5-base_* å¼€å¤´çš„æ¨¡å‹
    - å¿½ç•¥ JSON è¯„ä¼°ç»“æœæ–‡ä»¶
    - è‹¥å­˜åœ¨ `t5-small_tau_08/` è¿™æ ·çš„ç›®å½•ï¼Œä¼˜å…ˆåŠ è½½ `_100` ç‰ˆæœ¬
    """
    model_versions = []
    
    for model_name in os.listdir(models_dir):
        #print(f"model_name:{model_name}")
        model_path = os.path.join(models_dir, model_name)
        
        # åªé€‰å– T5 ç›¸å…³çš„æ¨¡å‹ç›®å½•ï¼Œæ’é™¤è¯„ä¼°æ–‡ä»¶
        if os.path.isdir(model_path) and (model_name.startswith("t5-small") or model_name.startswith("t5-base")):
            model_versions.append(model_name)
    
    # å¤„ç† `_100` ç‰ˆæœ¬ä¼˜å…ˆ
    final_model_list = []
    for model_name in model_versions:
        print(model_name)
        base_name = model_name.rstrip("_100").rstrip("_30").rstrip("_50").rstrip("_80")
        """ if base_name in model_versions and f"{base_name}_100" in model_versions:
            continue  # å¦‚æœ `_100` ç‰ˆæœ¬å­˜åœ¨ï¼Œåˆ™è·³è¿‡é `_100` ç‰ˆæœ¬ """
        final_model_list.append(model_name)
    
    return sorted(final_model_list)  # æ’åºï¼Œä¿è¯åŠ è½½é¡ºåºä¸€è‡´

model_versions = get_trained_models(models_dir)

print(f"\nğŸ“Œ å…±æ‰¾åˆ° {len(model_versions)} ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹:\n")
for model_version in model_versions:
    print(f" - {model_version}")

for model_version in model_versions:
    model_path = os.path.join(models_dir, model_version)

    if not os.path.exists(model_path):
        print(f"âš ï¸ æ¨¡å‹ {model_version} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ï¼")
        continue

    print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_version}")
    
    # **åŠ è½½ Tokenizer å’Œ Model**

"""     model_name = model_path
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(model_name)  # âœ… é‡æ–°ç”Ÿæˆ tokenizer.json

print("âœ… tokenizer.json é‡æ–°ç”Ÿæˆå®Œæˆï¼")
 """